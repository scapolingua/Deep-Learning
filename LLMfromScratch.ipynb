{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPduAzOQUSqJQ332/tlRo0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scapolingua/Deep-Learning/blob/master/LLMfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers datasets"
      ],
      "metadata": {
        "id": "egeUZmNzndOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyNm8Q0_nY72"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Carica subset Wikipedia IT\n",
        "dataset = load_dataset(\n",
        "    \"wikimedia/wikipedia\",\n",
        "    \"20231101.it\",\n",
        "    split=\"train[:5000]\"\n",
        ")\n",
        "\n",
        "# 2. Estrai testo\n",
        "texts = [row[\"text\"] for row in dataset]\n",
        "\n",
        "# 3. Costruisci tokenizer BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=8000,     # PERFETTO per SLM CPU\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(texts, trainer)\n",
        "\n",
        "# 4. Salva tokenizer\n",
        "tokenizer.save(\"tokenizer_it.json\")\n",
        "\n",
        "print(\"Tokenizer italiano BPE salvato\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "from datasets import load_dataset\n",
        "BLOCK_SIZE = 64\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"tokenizer_it.json\")\n",
        "\n",
        "def load_wiki_tokens(limit=5000):\n",
        "    dataset = load_dataset(\n",
        "        \"wikimedia/wikipedia\",\n",
        "        \"20231101.it\",\n",
        "        split=f\"train[:{limit}]\"\n",
        "    )\n",
        "\n",
        "    text = \"\\n\".join(row[\"text\"] for row in dataset)\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_batch(data, batch_size):\n",
        "    ix = torch.randint(0, len(data) - BLOCK_SIZE - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "8cVaCmz4jBMV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = load_wiki_tokens(5000)\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03qIrb2apm8Q",
        "outputId": "5fdae83a-7bdb-478e-880e-2ccb107906cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12188185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gen-0",
        "outputId": "ac10228c-aceb-4ab5-a046-982294657628"
      },
      "source": [
        "model_content = \"\"\"# model\n",
        "#from .config import BLOCK_SIZE\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = Attention(config)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        H = self.n_head\n",
        "        k = self.key(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "        q = self.query(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, -1e9)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        out = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "#------------------------------------\n",
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        block_size,\n",
        "        n_layer=4,\n",
        "        n_head=4,\n",
        "        n_embd=128,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "\n",
        "# -----------------------------\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(T)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1)\n",
        "            )\n",
        "            return logits, loss\n",
        "        return logits, None\n",
        "\"\"\"\n",
        "\n",
        "with open('model.py', 'w') as f:\n",
        "    f.write(model_content)\n",
        "\n",
        "print(\"File model.py aggiornato.\")\n",
        "\n",
        "data_content = \"\"\"import torch\n",
        "from tokenizers import Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "BLOCK_SIZE = 64\n",
        "tokenizer = Tokenizer.from_file(\"tokenizer_it.json\")\n",
        "\n",
        "def load_wiki_tokens(limit=5000):\n",
        "    dataset = load_dataset(\n",
        "        \"wikimedia/wikipedia\",\n",
        "        \"20231101.it\",\n",
        "        split=f\"train[:{limit}]\"\n",
        "    )\n",
        "\n",
        "    text = \"\\\\n\".join(row[\"text\"] for row in dataset)\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_batch(data, batch_size):\n",
        "    ix = torch.randint(0, len(data) - BLOCK_SIZE - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    return x, y\n",
        "\"\"\"\n",
        "\n",
        "with open('data.py', 'w') as f:\n",
        "    f.write(data_content)\n",
        "\n",
        "print(\"File data.py aggiornato.\")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import importlib\n",
        "\n",
        "# Create the checkpoints directory if it doesn't exist\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Reload the model and data modules to ensure latest changes are picked up\n",
        "import model\n",
        "import data\n",
        "importlib.reload(model)\n",
        "importlib.reload(data)\n",
        "from model import GPT, GPTConfig\n",
        "from data import load_wiki_tokens, get_batch\n",
        "\n",
        "# CONFIG\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS = 5000\n",
        "LR = 3e-4\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# DATA\n",
        "data = load_wiki_tokens(limit=5000)\n",
        "\n",
        "# MODEL\n",
        "config = GPTConfig(\n",
        "    vocab_size=12000,\n",
        "    block_size=128,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embd=256,\n",
        ")\n",
        "\n",
        "model = GPT(config).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "# TRAIN LOOP\n",
        "for step in range(1, MAX_STEPS + 1):\n",
        "    x, y = get_batch(data, BATCH_SIZE)\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    logits, loss = model(x, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"config\": config.__dict__,\n",
        "                \"step\": step,\n",
        "            },\n",
        "            f\"checkpoints/slm_step_{step}.pt\"\n",
        "        )\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File model.py aggiornato.\n",
            "File data.py aggiornato.\n",
            "Step 100 | Loss 7.2220\n",
            "Step 200 | Loss 7.1434\n",
            "Step 300 | Loss 7.0658\n",
            "Step 400 | Loss 6.9124\n",
            "Step 500 | Loss 6.7943\n",
            "Step 600 | Loss 6.7243\n",
            "Step 700 | Loss 6.7489\n",
            "Step 800 | Loss 6.6171\n",
            "Step 900 | Loss 6.3845\n",
            "Step 1000 | Loss 6.2078\n",
            "Step 1100 | Loss 6.2035\n",
            "Step 1200 | Loss 6.1577\n",
            "Step 1300 | Loss 6.0184\n",
            "Step 1400 | Loss 6.1968\n",
            "Step 1500 | Loss 6.1180\n",
            "Step 1600 | Loss 5.8781\n",
            "Step 1700 | Loss 5.7804\n",
            "Step 1800 | Loss 5.9131\n",
            "Step 1900 | Loss 5.8444\n",
            "Step 2000 | Loss 5.5914\n",
            "Step 2100 | Loss 5.8490\n",
            "Step 2200 | Loss 5.6483\n",
            "Step 2300 | Loss 6.0386\n",
            "Step 2400 | Loss 5.6167\n",
            "Step 2500 | Loss 5.6673\n",
            "Step 2600 | Loss 5.5264\n",
            "Step 2700 | Loss 5.4311\n",
            "Step 2800 | Loss 5.5736\n",
            "Step 2900 | Loss 5.5394\n",
            "Step 3000 | Loss 5.3187\n",
            "Step 3100 | Loss 5.4358\n",
            "Step 3200 | Loss 5.4304\n",
            "Step 3300 | Loss 5.3063\n",
            "Step 3400 | Loss 5.2417\n",
            "Step 3500 | Loss 5.4086\n",
            "Step 3600 | Loss 5.4485\n",
            "Step 3700 | Loss 5.2901\n",
            "Step 3800 | Loss 5.2434\n",
            "Step 3900 | Loss 5.3512\n",
            "Step 4000 | Loss 5.0721\n",
            "Step 4100 | Loss 5.2219\n",
            "Step 4200 | Loss 5.2626\n",
            "Step 4300 | Loss 5.2509\n",
            "Step 4400 | Loss 5.0816\n",
            "Step 4500 | Loss 5.3892\n",
            "Step 4600 | Loss 5.1878\n",
            "Step 4700 | Loss 5.1260\n",
            "Step 4800 | Loss 5.1153\n",
            "Step 4900 | Loss 5.0140\n",
            "Step 5000 | Loss 5.2114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59ebee6f",
        "outputId": "a1b00d3a-80e3-4b0f-a216-d3bb8e6862ff"
      },
      "source": [
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "from model import GPT, GPTConfig\n",
        "\n",
        "# Carica il tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"tokenizer_it.json\")\n",
        "\n",
        "# Carica il checkpoint del modello\n",
        "checkpoint_path = \"checkpoints/slm_step_5000.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "# Ricostruisci la configurazione del modello\n",
        "config = GPTConfig(**checkpoint['config'])\n",
        "\n",
        "# Inizializza il modello e carica lo stato\n",
        "model = GPT(config)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval() # Imposta il modello in modalità valutazione\n",
        "print(f\"Modello caricato con successo dal checkpoint: {checkpoint_path}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello caricato con successo dal checkpoint: checkpoints/slm_step_5000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfeba6ca",
        "outputId": "f5c6d01e-112f-446d-cd00-70b5814e1a1b"
      },
      "source": [
        "def generate_text(model, tokenizer, prompt, num_generate=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    # Codifica il prompt\n",
        "    encoded_prompt = tokenizer.encode(prompt).ids\n",
        "    context = torch.tensor(encoded_prompt, dtype=torch.long, device='cpu').unsqueeze(0)\n",
        "\n",
        "    generated_tokens = []\n",
        "    for _ in range(num_generate):\n",
        "        # Se il contesto supera la block_size, troncalo\n",
        "        if context.size(1) > model.config.block_size:\n",
        "            context = context[:, -model.config.block_size:]\n",
        "\n",
        "        # Ottieni i logits\n",
        "        logits, _ = model(context)\n",
        "        # Prendi l'ultimo timestep e applica la temperatura\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # Applica softmax per ottenere le probabilità\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        # Campiona dalla distribuzione\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Aggiungi il token generato alla sequenza\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "\n",
        "    # Decodifica la sequenza completa\n",
        "    full_sequence = encoded_prompt + generated_tokens\n",
        "    decoded_text = tokenizer.decode(full_sequence)\n",
        "    return decoded_text\n",
        "\n",
        "# Esempio di generazione di testo\n",
        "prompt = \"L'intelligenza artificiale\"\n",
        "generated_output = generate_text(model, tokenizer, prompt, num_generate=200, temperature=0.7)\n",
        "print(\"--- Testo generato ---\")\n",
        "print(generated_output)\n",
        "print(\"----------------------\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testo generato ---\n",
            "L ' intelli genza artificiale , il teorema di un periodo di un modello di coper tura di piante delle terre stri zioni . N et ano sono in cui la radiazione e l ' umani tà del gruppo permette di elementi della specie . La produzione di « Di re la scelta stra di una gestione del gruppo del più mar co . M ono no stro . È è una delle di proce dura di misura re del con una l ' si ha detto \" porta più di ma sche ma nero , data base di pro pa c ' allu se co di tra ina tradizionale o stra ona rie o to di una ri uscita negli d ' unica , un totale di un numero di ma aff in particolare A ere tto , ma ga , che , al qu er na za e pe ma sul di \" o non più o posto un ficato nato , in certi e R come gli A che , come di ogni tor nato che , i me do m che spa ma v eli cità di una o tri o A spe tto di sto ma in foto dal fo\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ec1fb0a"
      },
      "source": [
        "prompt_new = \"Il tempo è un concetto astratto e spesso difficile da definire, ma la sua importanza nella vita quotidiana è innegabile.\"\n",
        "generated_output_new = generate_text(model, tokenizer, prompt_new, num_generate=200, temperature=0.7)\n",
        "print(\"--- Testo generato con nuovo prompt ---\")\n",
        "print(generated_output_new)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "fea50616",
        "outputId": "56c57762-3d3a-4ff6-a02d-3dfc6ad6b3f9"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# 1. Carica un subset di validazione di Wikipedia IT\n",
        "# Usiamo una porzione diversa del dataset per la validazione\n",
        "validation_dataset = load_dataset(\n",
        "    \"wikimedia/wikipedia\",\n",
        "    \"20231101.it\",\n",
        "    split=\"train[5000:6000]\" # Esempio: usiamo 1000 esempi successivi\n",
        ")\n",
        "\n",
        "# 2. Estrai il testo e tokenizzalo usando il tokenizer addestrato\n",
        "validation_texts = [row[\"text\"] for row in validation_dataset]\n",
        "\n",
        "# Unisci il testo per la tokenizzazione come fatto per l'addestramento\n",
        "validation_tokens_ids = tokenizer.encode(\"\\n\".join(validation_texts)).ids\n",
        "\n",
        "# Converti in tensore PyTorch\n",
        "validation_data = torch.tensor(validation_tokens_ids, dtype=torch.long)\n",
        "\n",
        "print(f\"Numero totale di token nel set di validazione: {len(validation_data)}\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2585812670.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Unisci il testo per la tokenizzazione come fatto per l'addestramento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mvalidation_tokens_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Converti in tensore PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ba72ea3",
        "outputId": "0c66b64b-b6a4-4f44-e13e-87de1e44f74d"
      },
      "source": [
        "def calculate_perplexity(model, data, batch_size, block_size, device='cpu'):\n",
        "    model.eval() # Imposta il modello in modalità valutazione\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Non è necessario 'no_grad()' qui per lo stesso motivo che ho dato prima.\n",
        "    # Con 'no_grad' le operazioni verranno eseguite in modo più efficiente in termini di memoria e velocità,\n",
        "    # in quanto non sarà necessario calcolare i gradienti.\n",
        "    with torch.no_grad():\n",
        "        # Itera sui dati di validazione in batch\n",
        "        # Un modo semplice per creare batch sequenziali per la valutazione\n",
        "        for i in range(0, len(data) - block_size - 1, block_size):\n",
        "            x = data[i:i+block_size].unsqueeze(0).to(device) # Input del modello\n",
        "            y = data[i+1:i+block_size+1].unsqueeze(0).to(device) # Target\n",
        "\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "            total_loss += loss.item() * x.size(1) # Accumula la loss pesata per la lunghezza della sequenza\n",
        "            total_tokens += x.size(1)\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf') # Evita divisione per zero\n",
        "\n",
        "    average_loss = total_loss / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(average_loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "# Calcola la perplessità\n",
        "# Assicurati che BLOCK_SIZE del modello e della data siano gli stessi\n",
        "model_block_size = model.config.block_size\n",
        "perplexity = calculate_perplexity(model, validation_data, BATCH_SIZE, model_block_size, DEVICE)\n",
        "\n",
        "print(f\"Perplessità del modello sul set di validazione: {perplexity:.4f}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplessità del modello sul set di validazione: 309.3773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e852cf8",
        "outputId": "91ed1dc7-5ad5-4bdd-96b1-ca8565d6ec62"
      },
      "source": [
        "prompt_long = \"L'esplorazione spaziale, con i suoi incredibili progressi e le sfide che ancora attendono, cattura l'immaginazione di milioni di persone in tutto il mondo. Dalla corsa allo spazio degli anni '60 alla prospettiva di colonizzare Marte, l'umanità ha sempre guardato alle stelle con meraviglia e ambizione. Ogni nuova scoperta ci avvicina alla comprensione dell'universo, ma solleva anche nuove domande sulla nostra esistenza e sul nostro posto nel cosmo. La tecnologia avanza a passi da gigante, rendendo possibili missioni sempre più audaci e sofisticate. Quali segreti verranno svelati in futuro dalle profondità dello spazio?\"\n",
        "generated_output_long = generate_text(model, tokenizer, prompt_long, num_generate=250, temperature=0.8)\n",
        "print(\"--- Testo generato con prompt più lungo ---\")\n",
        "print(generated_output_long)\n",
        "print(\"------------------------------------------\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testo generato con prompt più lungo ---\n",
            "L ' esplo razione spaziale , con i suoi incre di bili progressi e le sfi de che ancora atten dono , cattura l ' immagina zione di milioni di persone in tutto il mondo . Dalla corsa allo spazio degli anni ' 60 alla prospe ttiva di coloni zzare Marte , l ' umani tà ha sempre guar dato alle stelle con m era vi glia e ambi zione . Ogni nuova scoperta ci av vicina alla compren sione dell ' universo , ma solle va anche nuove doman de sulla nostra esistenza e sul no stro posto nel co smo . La tecnologia avanza a passi da gigante , ren dendo possibili missioni sempre più au da ci e so fi stica te . Qu ali segre ti ver ranno s vel ati in futuro dalle profondità dello spazio ? I sto me ti vità sud classi ta x Monte cittadino e tor nata in tedesco e me ga ria a A tta , ma Sa pi è una era della cu cerca del gruppo mo a to mba , come la di cre o con l ' anti bano del ministro e mble ma che i ma i nella te me teori te x o più o ch di di un del DNA qui mb m elo vi colo o ma mmi dal o pen dice gol r un mor tran ne tta di fan dio a ed da al è uguale . Si di di Sim bo tta , che per la misura l l ' A tti o vi di una \" via e an one ( per far ri tu o si di 4 ma che ) o b una K o ke rio ' Ar me tta dramma tica ) oppure l W alter C une ma ge l presso A bo bo x ma ne a seguito la grande ad esempio H ad esempio la \" ma ( ap ena di un f ren tu m bol ma O ceano ). e can n spazio che ), l ' he ess ere dità di una pri vo ( S ( in e la w h y Ho ff o L ha ga a W hi mi ma N ori ma sempre i va i \" alc nato ricorda dal mon we i ciale )\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "f89793f3",
        "outputId": "280fb2d1-39ee-4edc-c5a0-297e430f7dc6"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "# Create the checkpoints directory if it doesn't exist\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Reload the model and data modules to ensure latest changes are picked up\n",
        "import model\n",
        "import data\n",
        "importlib.reload(model)\n",
        "importlib.reload(data)\n",
        "from model import GPT, GPTConfig\n",
        "from data import load_wiki_tokens, get_batch\n",
        "\n",
        "# CONFIG\n",
        "BATCH_SIZE = 16\n",
        "MAX_STEPS = 10000 # Increase MAX_STEPS for continued training example\n",
        "LR = 3e-4\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# DATA\n",
        "data = load_wiki_tokens(limit=5000)\n",
        "\n",
        "# MODEL\n",
        "# Initialize with a default config, will be overwritten if loading checkpoint\n",
        "config = GPTConfig(\n",
        "    vocab_size=12000,\n",
        "    block_size=128,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embd=256,\n",
        ")\n",
        "\n",
        "model = GPT(config).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "start_step = 1\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "checkpoint_path = \"checkpoints/slm_step_5000.pt\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Caricamento checkpoint da {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    # Re-instantiate config from checkpoint to ensure consistency\n",
        "    config = GPTConfig(**checkpoint['config'])\n",
        "    model = GPT(config).to(DEVICE)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    start_step = checkpoint['step'] + 1\n",
        "    # Optionally load optimizer state\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    print(f\"Addestramento ripreso dal passo {start_step}\")\n",
        "else:\n",
        "    print(\"Nessun checkpoint trovato, inizio addestramento da zero.\")\n",
        "\n",
        "# TRAIN LOOP\n",
        "for step in range(start_step, MAX_STEPS + 1):\n",
        "    x, y = get_batch(data, BATCH_SIZE)\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    logits, loss = model(x, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"config\": config.__dict__,\n",
        "                \"step\": step,\n",
        "            },\n",
        "            f\"checkpoints/slm_step_{step}.pt\"\n",
        "        )\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2518017804.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Reload the model and data modules to ensure latest changes are picked up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8facdadb",
        "outputId": "004c3488-64a6-4f74-b63c-5ad41820ec45"
      },
      "source": [
        "prompt_long = \"L'esplorazione spaziale, con i suoi incredibili progressi e le sfide che ancora attendono, cattura l'immaginazione di milioni di persone in tutto il mondo. Dalla corsa allo spazio degli anni '60 alla prospettiva di colonizzare Marte, l'umanità ha sempre guardato alle stelle con meraviglia e ambizione. Ogni nuova scoperta ci avvicina alla comprensione dell'universo, ma solleva anche nuove domande sulla nostra esistenza e sul nostro posto nel cosmo. La tecnologia avanza a passi da gigante, rendendo possibili missioni sempre più audaci e sofisticate. Quali segreti verranno svelati in futuro dalle profondità dello spazio?\"\n",
        "generated_output_low_temp = generate_text(model, tokenizer, prompt_long, num_generate=250, temperature=0.5)\n",
        "print(\"--- Testo generato con prompt lungo (temperature=0.5) ---\")\n",
        "print(generated_output_low_temp)\n",
        "print(\"----------------------------------------------------------\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testo generato con prompt lungo (temperature=0.5) ---\n",
            "L ' esplo razione spaziale , con i suoi incre di bili progressi e le sfi de che ancora atten dono , cattura l ' immagina zione di milioni di persone in tutto il mondo . Dalla corsa allo spazio degli anni ' 60 alla prospe ttiva di coloni zzare Marte , l ' umani tà ha sempre guar dato alle stelle con m era vi glia e ambi zione . Ogni nuova scoperta ci av vicina alla compren sione dell ' universo , ma solle va anche nuove doman de sulla nostra esistenza e sul no stro posto nel co smo . La tecnologia avanza a passi da gigante , ren dendo possibili missioni sempre più au da ci e so fi stica te . Qu ali segre ti ver ranno s vel ati in futuro dalle profondità dello spazio ? Z uri è la ma a una del sistema di una di un ga me tter ma ce m elo o per i con sor dio o di un ' a una delle più o in un cla endo per i server , e i bile in fi du i dio , la è uno , per i dio di una o il f at ch , che di che in una , ma glia ta , che ( o più o mi ma ) o o di o si ta e i ma che , o di di un o di una de cora bi e i ma b d ' a sia i ma che , e ne tta di una g ma ga b h ed è e i ver bi ologia o v ela bo e un ' A bi o ver bo m mo o ut ce m mo - o in una ( per la ten ne tta , o u do m uni taria . C S ing , e i car bu la o don a b rani ) o so o ma ). A ni ga va che si può essere un ra dice o i si può essere , i va di e co non è , che , che . L ' a sp p ton che sia per la no vo o h ) o h , nel \" ( o gn ( o po ur bi o\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88723e56",
        "outputId": "68315475-c449-402e-e5aa-cc2c7018b76e"
      },
      "source": [
        "prompt_long = \"L'esplorazione spaziale, con i suoi incredibili progressi e le sfide che ancora attendono, cattura l'immaginazione di milioni di persone in tutto il mondo. Dalla corsa allo spazio degli anni '60 alla prospettiva di colonizzare Marte, l'umanità ha sempre guardato alle stelle con meraviglia e ambizione. Ogni nuova scoperta ci avvicina alla comprensione dell'universo, ma solleva anche nuove domande sulla nostra esistenza e sul nostro posto nel cosmo. La tecnologia avanza a passi da gigante, rendendo possibili missioni sempre più audaci e sofisticate. Quali segreti verranno svelati in futuro dalle profondità dello spazio?\"\n",
        "generated_output_long = generate_text(model, tokenizer, prompt_long, num_generate=250, temperature=0.8)\n",
        "print(\"--- Testo generato con prompt più lungo e modello aggiornato ---\")\n",
        "print(generated_output_long)\n",
        "print(\"----------------------------------------------------------\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testo generato con prompt più lungo e modello aggiornato ---\n",
            "L ' esplo razione spaziale , con i suoi incre di bili progressi e le sfi de che ancora atten dono , cattura l ' immagina zione di milioni di persone in tutto il mondo . Dalla corsa allo spazio degli anni ' 60 alla prospe ttiva di coloni zzare Marte , l ' umani tà ha sempre guar dato alle stelle con m era vi glia e ambi zione . Ogni nuova scoperta ci av vicina alla compren sione dell ' universo , ma solle va anche nuove doman de sulla nostra esistenza e sul no stro posto nel co smo . La tecnologia avanza a passi da gigante , ren dendo possibili missioni sempre più au da ci e so fi stica te . Qu ali segre ti ver ranno s vel ati in futuro dalle profondità dello spazio ? ( l ' 11 bi zione della dice f na scente tto ine i ferra mentare ), e del ca una spin it ma mmi che le fu un va inven zione della bi ologia del cor dia della classe proveni to da pa ta mento con la ma ris pose a bo tte nel che , i ma che la cui in seguito al ber me f ona mento b anza ) & C ' O NU da cui è definita come un men si ta glia esp or ren e che de vel tra dotto con versione di un ' ur bi ologia nelle un us , la loro delle stesse per il na a settembre e geo grafico di una y me Famiglia sulla dimensi onale ma ca va , Is and ere sia go o So phi y thon o A du ves finale , . Nel il ber - ar , a bilità di da tta S al bi zu ba per il , dove sp un zo , poi a gente del No u ma glia ma , cominciò p f ren dipen taria e giochi che i ma Giorgio - un gh ch , in studio ther e sione Ch ampi co b bo – ere co to Giacomo era I p porto na anese a chino e vi la gimento finale John Tor rio - Ste al me x ma positivo e co dici me del locale fu tu bo e bo\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}